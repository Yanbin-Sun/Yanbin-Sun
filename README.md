# Yanbin Sun

Master Student in Artificial Intelligence  
Focus on Multimodal Learning and Generative Models  

---

## ðŸ”¬ Research Interests

- Diffusion Models
- Emotion-aware Image Generation
- Vision-Language Models (VLM)
- Multimodal Representation Alignment
- Digital Human Systems

---

## ðŸ§  Current Work

I am currently working on emotion-conditioned diffusion models and multimodal evaluation pipelines.

My research focuses on:

- Training and adapting diffusion models (e.g., SD3.5 / Flow-based models)
- Emotion alignment in image generation
- Vision-language based evaluation and filtering
- Multimodal representation consistency

---

## ðŸ›  Technical Skills

- PyTorch / Transformers / Diffusers
- Multi-GPU Distributed Training
- LoRA Fine-tuning
- Vision-Language Model Integration
- Python / Linux / HPC

---

## ðŸ“Š Research Direction

I am particularly interested in:

- How emotional signals can guide generative models
- Alignment between semantic prompts and visual outputs
- Evaluation metrics for generative quality and emotional consistency
- Efficient adaptation of large-scale diffusion systems

---

## ðŸ“« Contact

Email: 13153021897@163.com  
GitHub: https://github.com/Yanbin-Sun  

---

> This page is used as an academic profile for research-related activities.
